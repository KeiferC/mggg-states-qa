{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining Wikipedia Election Data\n",
    "=========================\n",
    "\n",
    "This notebook mines the data in the Wikipedia pages for the following elections:\n",
    "\n",
    "- 2016 United States presidential election\n",
    "- 2016 United States Senate elections\n",
    "- 2016 United States House of Representatives elections\n",
    "- 2017 United States Senate elections (special elections)\n",
    "- 2018 United States Senate elections\n",
    "- 2018 United States House of Representatives elections\n",
    "\n",
    "Timestamp: 12:00 PM ET, 11 Aug. 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install wikipedia\n",
    "\n",
    "# !pip3 install git+https://github.com/KeiferC/gdutils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import os\n",
    "import re\n",
    "\n",
    "import gdutils.datamine as dm\n",
    "import gdutils.dataqa as dq\n",
    "import gdutils.extract as et\n",
    "\n",
    "from typing import Any, List, Tuple, Dict, Hashable, Union, NoReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = [\n",
    "    'Alabama', 'Alaska','Arizona', 'Arkansas', 'California', \n",
    "    'Colorado', 'Connecticut', 'Delaware',  'Florida', 'Georgia', \n",
    "    'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', \n",
    "    'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', \n",
    "    'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', \n",
    "    'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', \n",
    "    'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', \n",
    "    'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "state_abbreviations = [\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', \n",
    "    'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', \n",
    "    'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', \n",
    "    'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', \n",
    "    'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', \n",
    "    'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "states = list(zip(state_names, state_abbreviations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Generate Wikipedia page titles for scraping\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pres_election = ('PRES', 'United States presidential election')\n",
    "\n",
    "# fed_elections = [('SEN',  'United States Senate election'),\n",
    "#                  ('USH',  'United States House of Representatives election')]\n",
    "\n",
    "# election_years_to_check = [2016, 2017, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_titles = []\n",
    "\n",
    "# for yr in election_years_to_check:\n",
    "#     generate_key = lambda yr, ekey, st_abv: ekey + str(yr % 100) + '_' + st_abv\n",
    "#     generate_title = lambda yr, etype, st: str(yr) + ' ' + etype + ' in ' + st\n",
    "    \n",
    "#     if yr % 4 == 0:\n",
    "#         [wiki_titles.append((generate_key(yr, pres_election[0], st_abv),\n",
    "#                              generate_title(yr, pres_election[1], st)))\n",
    "#          for st, st_abv in states]\n",
    "        \n",
    "#     [wiki_titles.append((generate_key(yr, ekey, st_abv),\n",
    "#                          generate_title(yr, etype, st)))\n",
    "#      for ekey, etype in fed_elections\n",
    "#      for st, st_abv in states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Gather Wikipedia URLs from page titles\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_urls = {}\n",
    "\n",
    "# for wiki_title in wiki_titles:\n",
    "#     key, title = wiki_title\n",
    "    \n",
    "#     try:\n",
    "#         url = wikipedia.page(title=title).url\n",
    "\n",
    "#         if set(title.split(' ')).issubset(\n",
    "#                 set(re.findall('[a-zA-Z0-9]+', url))):\n",
    "#             wiki_urls[key] = (title, url)\n",
    "            \n",
    "#     except Exception:\n",
    "#         continue # it's okay if page does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print retrieved page URLs\n",
    "# Necessary for manually verifying URL-to-election mapping since\n",
    "# Wikipedia API tries to find best match, not the exact match\n",
    "\n",
    "# for wiki_key in wiki_urls:\n",
    "#     title, url = wiki_urls[wiki_key]\n",
    "#     print('{:9} : {}\\n\\t{}'.format(wiki_key, title, url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Gather tabular data from Wikipedia pages\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_tables = {}\n",
    "\n",
    "# for wiki_key in wiki_urls:\n",
    "#     try:\n",
    "#         wiki_tables[wiki_key] = pd.read_html(wiki_urls[wiki_key][1])\n",
    "#     except Exception as e:\n",
    "#         print(\"Unable to gather Wikipedia tabular data:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wikipedia tabular election data\n",
    "# Necessary for finding applicable table because a page can \n",
    "# contain multiple nameless tables whose orders differ from\n",
    "# other pages\n",
    "\n",
    "# def print_wiki_tables(key):\n",
    "#     for wiki in wiki_tables:\n",
    "#         if wiki.startswith(key):\n",
    "#             print('================================================')\n",
    "#             print('Wiki: {} '.format(wiki))\n",
    "#             print('================================================')\n",
    "\n",
    "#             for i in range(len(wiki_tables[wiki])):\n",
    "#                 print('TABLE {}: ############################\\n{}\\n\\n\\n'.format(\n",
    "#                         i, wiki_tables[wiki][i].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('PRES16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('SEN16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('USH16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('SEN17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('USH17') # no data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('SEN18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out to save screen space\n",
    "# print_wiki_tables('USH18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Collect applicable election data from tables\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has to be done manually because every Wikipedia page \n",
    "# is different and because Wikipedia doesn't have datasets\n",
    "# to download and thus all tables are scraped\n",
    "\n",
    "# wiki_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2016 United States presidential election data (``PRES16``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unless otherwise stated, all data below are at the county-level\n",
    "\n",
    "# wiki_dfs['PRES16_AL'] = wiki_tables['PRES16_AL'][6]  \n",
    "# wiki_dfs['PRES16_AK'] = wiki_tables['PRES16_AK'][17] # state-level\n",
    "# wiki_dfs['PRES16_AZ'] = wiki_tables['PRES16_AZ'][20] \n",
    "# wiki_dfs['PRES16_AR'] = wiki_tables['PRES16_AR'][8]  \n",
    "# wiki_dfs['PRES16_CA'] = wiki_tables['PRES16_CA'][36] \n",
    "# wiki_dfs['PRES16_CO'] = wiki_tables['PRES16_CO'][21] \n",
    "# wiki_dfs['PRES16_CT'] = wiki_tables['PRES16_CT'][16] \n",
    "# wiki_dfs['PRES16_DE'] = wiki_tables['PRES16_DE'][13] \n",
    "# wiki_dfs['PRES16_FL'] = wiki_tables['PRES16_FL'][14] \n",
    "# wiki_dfs['PRES16_GA'] = wiki_tables['PRES16_GA'][13] \n",
    "# wiki_dfs['PRES16_HI'] = wiki_tables['PRES16_HI'][12] \n",
    "# wiki_dfs['PRES16_ID'] = wiki_tables['PRES16_ID'][16] \n",
    "# wiki_dfs['PRES16_IL'] = wiki_tables['PRES16_IL'][26] \n",
    "# wiki_dfs['PRES16_IN'] = wiki_tables['PRES16_IN'][14] \n",
    "# wiki_dfs['PRES16_IA'] = wiki_tables['PRES16_IA'][13] \n",
    "# wiki_dfs['PRES16_KS'] = wiki_tables['PRES16_KS'][17] \n",
    "# wiki_dfs['PRES16_KY'] = wiki_tables['PRES16_KY'][12] \n",
    "# wiki_dfs['PRES16_LA'] = wiki_tables['PRES16_LA'][9]  # parish-level\n",
    "# wiki_dfs['PRES16_ME'] = wiki_tables['PRES16_ME'][19] \n",
    "# wiki_dfs['PRES16_MD'] = wiki_tables['PRES16_MD'][15] \n",
    "# wiki_dfs['PRES16_MA'] = wiki_tables['PRES16_MA'][16] \n",
    "# wiki_dfs['PRES16_MI'] = wiki_tables['PRES16_MI'][17] \n",
    "# wiki_dfs['PRES16_MN'] = wiki_tables['PRES16_MN'][18] \n",
    "# wiki_dfs['PRES16_MS'] = wiki_tables['PRES16_MS'][17] \n",
    "# wiki_dfs['PRES16_MO'] = wiki_tables['PRES16_MO'][16] \n",
    "# wiki_dfs['PRES16_MT'] = wiki_tables['PRES16_MT'][10] \n",
    "# wiki_dfs['PRES16_NE'] = wiki_tables['PRES16_NE'][26] \n",
    "# wiki_dfs['PRES16_NV'] = wiki_tables['PRES16_NV'][17] \n",
    "# wiki_dfs['PRES16_NH'] = wiki_tables['PRES16_NH'][20] \n",
    "# wiki_dfs['PRES16_NJ'] = wiki_tables['PRES16_NJ'][14] \n",
    "# wiki_dfs['PRES16_NM'] = wiki_tables['PRES16_NM'][13] \n",
    "# wiki_dfs['PRES16_NY'] = wiki_tables['PRES16_NY'][27] \n",
    "# wiki_dfs['PRES16_NC'] = wiki_tables['PRES16_NC'][18] \n",
    "# wiki_dfs['PRES16_ND'] = wiki_tables['PRES16_ND'][12] \n",
    "# wiki_dfs['PRES16_OH'] = wiki_tables['PRES16_OH'][23] \n",
    "# wiki_dfs['PRES16_OK'] = wiki_tables['PRES16_OK'][20] \n",
    "# wiki_dfs['PRES16_OR'] = wiki_tables['PRES16_OR'][21] \n",
    "# wiki_dfs['PRES16_PA'] = wiki_tables['PRES16_PA'][19] \n",
    "# wiki_dfs['PRES16_RI'] = wiki_tables['PRES16_RI'][12] \n",
    "# wiki_dfs['PRES16_SC'] = wiki_tables['PRES16_SC'][16] \n",
    "# wiki_dfs['PRES16_SD'] = wiki_tables['PRES16_SD'][11] \n",
    "# wiki_dfs['PRES16_TN'] = wiki_tables['PRES16_TN'][13] \n",
    "# wiki_dfs['PRES16_TX'] = wiki_tables['PRES16_TX'][29] \n",
    "# wiki_dfs['PRES16_UT'] = wiki_tables['PRES16_UT'][13] \n",
    "# wiki_dfs['PRES16_VT'] = wiki_tables['PRES16_VT'][16] \n",
    "# wiki_dfs['PRES16_VA'] = wiki_tables['PRES16_VA'][20] \n",
    "# wiki_dfs['PRES16_WA'] = wiki_tables['PRES16_WA'][13] \n",
    "# wiki_dfs['PRES16_WV'] = wiki_tables['PRES16_WV'][11] \n",
    "# wiki_dfs['PRES16_WI'] = wiki_tables['PRES16_WI'][16] \n",
    "# wiki_dfs['PRES16_WY'] = wiki_tables['PRES16_WY'][12] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2016 United States Senate election data (``SEN16``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All election data below are at the State-level\n",
    "\n",
    "# wiki_dfs['SEN16_AL'] = wiki_tables['SEN16_AL'][19] \n",
    "# wiki_dfs['SEN16_AK'] = wiki_tables['SEN16_AK'][20] \n",
    "# wiki_dfs['SEN16_AZ'] = wiki_tables['SEN16_AZ'][45]\n",
    "# wiki_dfs['SEN16_AR'] = wiki_tables['SEN16_AR'][16]\n",
    "# wiki_dfs['SEN16_CA'] = wiki_tables['SEN16_CA'][53]\n",
    "# wiki_dfs['SEN16_CO'] = wiki_tables['SEN16_CO'][25]\n",
    "# wiki_dfs['SEN16_CT'] = wiki_tables['SEN16_CT'][20]\n",
    "# wiki_dfs['SEN16_FL'] = wiki_tables['SEN16_FL'][64]\n",
    "# wiki_dfs['SEN16_GA'] = wiki_tables['SEN16_GA'][16]\n",
    "# wiki_dfs['SEN16_HI'] = wiki_tables['SEN16_HI'][18]\n",
    "# wiki_dfs['SEN16_ID'] = wiki_tables['SEN16_ID'][15]\n",
    "# wiki_dfs['SEN16_IL'] = wiki_tables['SEN16_IL'][29]\n",
    "# wiki_dfs['SEN16_IN'] = wiki_tables['SEN16_IN'][25]\n",
    "# wiki_dfs['SEN16_IA'] = wiki_tables['SEN16_IA'][20]\n",
    "# wiki_dfs['SEN16_KS'] = wiki_tables['SEN16_KS'][17]\n",
    "# wiki_dfs['SEN16_KY'] = wiki_tables['SEN16_KY'][22]\n",
    "# wiki_dfs['SEN16_LA'] = wiki_tables['SEN16_LA'][24]\n",
    "# wiki_dfs['SEN16_MD'] = wiki_tables['SEN16_MD'][29]\n",
    "# wiki_dfs['SEN16_MO'] = wiki_tables['SEN16_MO'][24]\n",
    "# wiki_dfs['SEN16_NV'] = wiki_tables['SEN16_NV'][32]\n",
    "# wiki_dfs['SEN16_NH'] = wiki_tables['SEN16_NH'][23]\n",
    "# wiki_dfs['SEN16_NY'] = wiki_tables['SEN16_NY'][15]\n",
    "# wiki_dfs['SEN16_NC'] = wiki_tables['SEN16_NC'][42]\n",
    "# wiki_dfs['SEN16_ND'] = wiki_tables['SEN16_ND'][14]\n",
    "# wiki_dfs['SEN16_OH'] = wiki_tables['SEN16_OH'][29]\n",
    "# wiki_dfs['SEN16_OK'] = wiki_tables['SEN16_OK'][12]\n",
    "# wiki_dfs['SEN16_OR'] = wiki_tables['SEN16_OR'][14]\n",
    "# wiki_dfs['SEN16_PA'] = wiki_tables['SEN16_PA'][38]\n",
    "# wiki_dfs['SEN16_SC'] = wiki_tables['SEN16_SC'][16]\n",
    "# wiki_dfs['SEN16_SD'] = wiki_tables['SEN16_SD'][9]\n",
    "# wiki_dfs['SEN16_UT'] = wiki_tables['SEN16_UT'][19]\n",
    "# wiki_dfs['SEN16_VT'] = wiki_tables['SEN16_VT'][12]\n",
    "# wiki_dfs['SEN16_WA'] = wiki_tables['SEN16_WA'][15]\n",
    "# wiki_dfs['SEN16_WI'] = wiki_tables['SEN16_WI'][21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2016 United States House of Representative election data (``USH16``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All election data below are at the State-level\n",
    "\n",
    "# wiki_dfs['USH16_AK'] = wiki_tables['USH16_AK'][15]\n",
    "# wiki_dfs['USH16_DE'] = wiki_tables['USH16_DE'][17]\n",
    "# wiki_dfs['USH16_MT'] = wiki_tables['USH16_MT'][10]\n",
    "# wiki_dfs['USH16_ND'] = wiki_tables['USH16_ND'][11]\n",
    "# wiki_dfs['USH16_SD'] = wiki_tables['USH16_SD'][8]\n",
    "# wiki_dfs['USH16_VT'] = wiki_tables['USH16_VT'][10]\n",
    "# wiki_dfs['USH16_WY'] = wiki_tables['USH16_WY'][21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2017 United States Senate election data (``SEN17``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All election data below are at the State-level\n",
    "\n",
    "# wiki_dfs['SEN17_AL'] = wiki_tables['SEN17_AL'][48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2017 United States House of Representative election data (``USH17``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No such data exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2018 United States Senate election data (``SEN18``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All election data below are at the State-level\n",
    "\n",
    "# wiki_dfs['SEN18_AZ'] = wiki_tables['SEN18_AZ'][40]\n",
    "# wiki_dfs['SEN18_CA'] = wiki_tables['SEN18_CA'][54]\n",
    "# wiki_dfs['SEN18_CT'] = wiki_tables['SEN18_CT'][17] \n",
    "# wiki_dfs['SEN18_DE'] = wiki_tables['SEN18_DE'][29]\n",
    "# wiki_dfs['SEN18_FL'] = wiki_tables['SEN18_FL'][29]\n",
    "# wiki_dfs['SEN18_HI'] = wiki_tables['SEN18_HI'][14]\n",
    "# wiki_dfs['SEN18_IN'] = wiki_tables['SEN18_IN'][29]\n",
    "# wiki_dfs['SEN18_ME'] = wiki_tables['SEN18_ME'][20]\n",
    "# wiki_dfs['SEN18_MD'] = wiki_tables['SEN18_MD'][25]\n",
    "# wiki_dfs['SEN18_MA'] = wiki_tables['SEN18_MA'][29]\n",
    "# wiki_dfs['SEN18_MI'] = wiki_tables['SEN18_MI'][30]\n",
    "# wiki_dfs['SEN18_MN'] = wiki_tables['SEN18_MN'][20]\n",
    "# wiki_dfs['SEN18_MS'] = wiki_tables['SEN18_MS'][23]\n",
    "# wiki_dfs['SEN18_MO'] = wiki_tables['SEN18_MO'][35]\n",
    "# wiki_dfs['SEN18_MT'] = wiki_tables['SEN18_MT'][22]\n",
    "# wiki_dfs['SEN18_NE'] = wiki_tables['SEN18_NE'][19]\n",
    "# wiki_dfs['SEN18_NV'] = wiki_tables['SEN18_NV'][28]\n",
    "# wiki_dfs['SEN18_NJ'] = wiki_tables['SEN18_NJ'][22]\n",
    "# wiki_dfs['SEN18_NM'] = wiki_tables['SEN18_NM'][21]\n",
    "# wiki_dfs['SEN18_NY'] = wiki_tables['SEN18_NY'][16]\n",
    "# wiki_dfs['SEN18_ND'] = wiki_tables['SEN18_ND'][23]\n",
    "# wiki_dfs['SEN18_OH'] = wiki_tables['SEN18_OH'][32]\n",
    "# wiki_dfs['SEN18_PA'] = wiki_tables['SEN18_PA'][28]\n",
    "# wiki_dfs['SEN18_RI'] = wiki_tables['SEN18_RI'][17]\n",
    "# wiki_dfs['SEN18_TN'] = wiki_tables['SEN18_TN'][29]\n",
    "# wiki_dfs['SEN18_TX'] = wiki_tables['SEN18_TX'][37]\n",
    "# wiki_dfs['SEN18_UT'] = wiki_tables['SEN18_UT'][31]\n",
    "# wiki_dfs['SEN18_VT'] = wiki_tables['SEN18_VT'][13]\n",
    "# wiki_dfs['SEN18_VA'] = wiki_tables['SEN18_VA'][32]\n",
    "# wiki_dfs['SEN18_WA'] = wiki_tables['SEN18_WA'][12]\n",
    "# wiki_dfs['SEN18_WV'] = wiki_tables['SEN18_WV'][31]\n",
    "# wiki_dfs['SEN18_WI'] = wiki_tables['SEN18_WI'][27]\n",
    "# wiki_dfs['SEN18_WY'] = wiki_tables['SEN18_WY'][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2018 United States House of Representative election data (``USH18``)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All election data below are at the State-level\n",
    "\n",
    "# wiki_dfs['USH18_AK'] = wiki_tables['USH18_AK'][14]\n",
    "# wiki_dfs['USH18_DE'] = wiki_tables['USH18_DE'][19]\n",
    "# wiki_dfs['USH18_MT'] = wiki_tables['USH18_MT'][13]\n",
    "# wiki_dfs['USH18_ND'] = wiki_tables['USH18_ND'][15]\n",
    "# wiki_dfs['USH18_SD'] = wiki_tables['USH18_SD'][11]\n",
    "# wiki_dfs['USH18_VT'] = wiki_tables['USH18_VT'][12]\n",
    "# wiki_dfs['USH18_WY'] = wiki_tables['USH18_WY'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Save raw scraped tables locally\n",
    "----------------------------------------------\n",
    "\n",
    "For future auditing and granular data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for election in wiki_dfs:\n",
    "#     outpath = os.path.join('wiki', 'raw', election + '.csv')\n",
    "#     et.ExtractTable(wiki_dfs[election], \n",
    "#                     outfile=outpath).extract_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Wrangle Wikipedia data\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_key_from_candidate = {\n",
    "    'CLINTON' : 'D',\n",
    "    'HILLARY' : 'D',\n",
    "    'RODHAM'  : 'D',\n",
    "\n",
    "    'JOHNSON' : 'L',\n",
    "    'GARY'    : 'L',\n",
    "    \n",
    "    'STEIN'   : 'G',\n",
    "    'JILL'    : 'G',\n",
    "    'Ellen'   : 'G',\n",
    "    \n",
    "    'TRUMP'   : 'R',\n",
    "    'DONALD'  : 'R',\n",
    "    'JOHN'    : 'R'\n",
    "}\n",
    "\n",
    "party_key_from_party = {\n",
    "    'DEMOCRATIC'    : 'D',\n",
    "    'DEMOCRAT'      : 'D',\n",
    "    'DEMOCRATIC-NP' : 'D',\n",
    "    'GREEN'         : 'G',\n",
    "    'LIBERTARIAN'   : 'L',\n",
    "    'REPUBLICAN'    : 'R'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_files = dm.list_files_of_type('.csv', os.path.join('wiki', 'raw'))\n",
    "\n",
    "wiki_dfs = {}\n",
    "for file in wiki_files:\n",
    "    wiki_dfs[os.path.basename(file)[:-4]] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_keys = list(party_key_from_candidate.keys())\n",
    "party_keys = list(party_key_from_party.keys())\n",
    "\n",
    "# tries to rename columns to fit keys while finding\n",
    "# right row to use as columns\n",
    "def rename_raw_cols(df, election_key) -> pd.DataFrame:\n",
    "    new_cols = {}\n",
    "\n",
    "    for name in df.columns:\n",
    "        alpha_cols = re.findall('[a-zA-Z0-9]+', name) \n",
    "\n",
    "        for col in alpha_cols:\n",
    "            if col in candidate_keys or col in party_keys:\n",
    "                new_cols[name] = col\n",
    "                break\n",
    "\n",
    "    if (not new_cols and election_key.startswith('PRES') and\n",
    "        not election_key.endswith('AK')):\n",
    "        df.columns = [str(x) for x in df.iloc[0].tolist()]\n",
    "        df.columns = upper_cols(df)\n",
    "        df = df.drop(df.index[0])\n",
    "        \n",
    "        return rename_raw_cols(df, election_key)\n",
    "    \n",
    "    else:\n",
    "        return df.rename(columns=new_cols)\n",
    "\n",
    "    \n",
    "def upper_cols(df: pd.DataFrame) -> List[str]:\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            cols.append(col.upper().strip())\n",
    "        except:\n",
    "            cols.append(str(col))\n",
    "            \n",
    "    return cols\n",
    "\n",
    "\n",
    "# change column names to standard casing\n",
    "def keyify_and_upper_cols():\n",
    "    for election in wiki_dfs:\n",
    "        df = wiki_dfs[election]\n",
    "        df.columns = upper_cols(df)\n",
    "                \n",
    "        wiki_dfs[election].columns = df.columns\n",
    "        wiki_dfs[election] = rename_raw_cols(df, election)\n",
    "\n",
    "    \n",
    "# change rows to standard casing\n",
    "def upper_rows():\n",
    "    for election in wiki_dfs:\n",
    "        df = wiki_dfs[election]\n",
    "\n",
    "        for name in df.columns:\n",
    "            try:\n",
    "                df[name] = df[name].str.upper()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        wiki_dfs[election] = df\n",
    "\n",
    "        \n",
    "keyify_and_upper_cols()\n",
    "upper_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize pres data\n",
    "\n",
    "for election in wiki_dfs:\n",
    "    if election.startswith('PRES'):\n",
    "        pass\n",
    "        # TODO -- get vote counts only -- no percentages\n",
    "#         df = wiki_dfs[election]\n",
    "#         df = df.loc[df[]]\n",
    "\n",
    "#         try:\n",
    "#             df = wiki_dfs[election].select_dtypes(['number'])\n",
    "#         except:\n",
    "#             print('not working')\n",
    "#         print(election, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp storage of partially processed raw\n",
    "\n",
    "# for election in wiki_dfs:\n",
    "#     outpath = os.path.join('wiki', 'temp', election + '.csv')\n",
    "#     et.ExtractTable(wiki_dfs[election], \n",
    "#                     outfile=outpath).extract_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_tuple(key: str) -> Tuple[str, str]:\n",
    "    key_components = key.split('_')\n",
    "    return (key_components[0], key_components[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_tuple(state_key: str) -> Tuple[str, str]:\n",
    "    state_tup = [tup for tup in states if tup[1] == state_key]\n",
    "    state_name, state_abv = state_tup[0]\n",
    "    return (state_name.upper(), state_abv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empty_standardized_df(state_name: str) -> pd.DataFrame:\n",
    "    standardized_cols = ['STATE', \n",
    "                         'PRES16D', 'PRES16G', 'PRES16L', 'PRES16R',\n",
    "                         'SEN16D',  'SEN16G',  'SEN16L',  'SEN16R',\n",
    "                         'USH16D',  'USH16G',  'USH16L',  'USH16R',\n",
    "                         'SEN17D',  'SEN17G',  'SEN17L',  'SEN17R',\n",
    "                         'USH17D',  'USH17G',  'USH17L',  'USH17R',\n",
    "                         'SEN18D',  'SEN18G',  'SEN18L',  'SEN18R',\n",
    "                         'USH18D',  'USH18G',  'USH18L',  'USH18R']\n",
    "    \n",
    "    standardized_data = [[state_name,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan,\n",
    "                         np.nan, np.nan, np.nan, np.nan]]\n",
    "    \n",
    "    df = pd.DataFrame(columns=standardized_cols, data=standardized_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_df(standardized, state_name) -> pd.DataFrame:\n",
    "    try:\n",
    "        return standardized[state_name]\n",
    "    except:\n",
    "        standardized[state_name] = generate_empty_standardized_df(state_name)\n",
    "        return standardized[state_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-455355f198a6>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-455355f198a6>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    rows_df = df.loc[df[]]\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_wiki_pres(standardized: pd.DataFrame, pres_df: pd.DataFrame\n",
    "        ) -> pd.DataFrame:\n",
    "    df = standardized.copy()\n",
    "\n",
    "    if df['STATE'] == 'ALASKA':\n",
    "        rows_df = df.loc[df[]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_federal(standardized: pd.DataFrame, fed_df: pd.DataFrame\n",
    "        ) -> pd.DataFrame:\n",
    "    df = standardized.copy()\n",
    "    #TODO\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Format for each standardized DataFrame:\n",
    "\n",
    "STATE   | PRES16D | PRES16G | PRES16L | PRES16R | SEN16D | SEN16G | ... | USH18R\n",
    "---------------------------------------------------------------------------------\n",
    "ALABAMA | ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "standardized_wiki_dfs = {}\n",
    "\n",
    "for wiki_key in wiki_dfs:\n",
    "    election_key, state_key = get_key_tuple(wiki_key)\n",
    "    state_name, state_abv = get_state_tuple(state_key)\n",
    "    \n",
    "    standardized_df = get_standardized_df(standardized_wiki_dfs, state_name)\n",
    "    \n",
    "    if election_key.startswith('PRES'):\n",
    "        print('PRES: TODO -', election_key)\n",
    "    elif election_key.startswith('SEN') or\n",
    "         election_key.startswith('USH'):\n",
    "        print('TODO -', election_key)\n",
    "    else:\n",
    "        print('Election not currently used:', election_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Format for concatenated standardized DataFrame:\n",
    "\n",
    "STATE   | PRES16D | PRES16G | PRES16L | PRES16R | SEN16D | SEN16G | ... | USH18R\n",
    "---------------------------------------------------------------------------------\n",
    "ALABAMA | ...\n",
    "ALASKA  | ...\n",
    "...\n",
    "WYOMING | ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "wiki_states_df = pd.DataFrame()\n",
    "for key in standardized_wiki_dfs:\n",
    "    wiki_states_df = pd.concat([wiki_states_df, standardized_wiki_dfs[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. Save processed Wikipedia data locally\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_df_outfile = os.path.join('wiki', 'wiki_states.csv')\n",
    "\n",
    "# wiki_et = et.ExtractTable(wiki_states_df, column='STATE', \n",
    "#                 outfile=wiki_df_outfile).extract_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
